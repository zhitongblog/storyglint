/**
 * OpenAI GPT AI Provider
 * åŸºäº OpenAI APIï¼ˆä½¿ç”¨ Electron IPC ä»£ç†è¯·æ±‚ï¼‰
 */

import type { AIProvider, ModelInfo, QuotaInfo, ProviderMeta } from '../types'
import { PROVIDER_INFO } from '../types'
import { aiFetch } from '../fetch-helper'

// æµå¼è¯·æ±‚ä½¿ç”¨åŸç”Ÿ fetchï¼ˆä»…åœ¨ Electron ç¯å¢ƒä¸­é€šè¿‡ session ä»£ç†ï¼‰
// éæµå¼è¯·æ±‚ä½¿ç”¨ aiFetch é€šè¿‡ IPC ä»£ç†

// OpenAI æ¨¡å‹é…ç½®
const OPENAI_MODELS: Record<string, ModelInfo> = {
  'gpt-4o': {
    name: 'GPT-4o',
    description: 'æœ€æ–°æ——èˆ°æ¨¡å‹ï¼Œå¤šæ¨¡æ€èƒ½åŠ›å¼ºï¼Œæ€§ä»·æ¯”é«˜',
    contextWindow: 128000,
    recommended: true
  },
  'gpt-4o-mini': {
    name: 'GPT-4o Mini',
    description: 'å°å‹ç‰ˆæœ¬ï¼Œé€Ÿåº¦å¿«ï¼Œæˆæœ¬ä½',
    contextWindow: 128000,
    recommended: false
  },
  'gpt-4-turbo': {
    name: 'GPT-4 Turbo',
    description: 'é«˜æ€§èƒ½ç‰ˆæœ¬ï¼Œæ”¯æŒè§†è§‰è¾“å…¥',
    contextWindow: 128000,
    recommended: false
  },
  'gpt-3.5-turbo': {
    name: 'GPT-3.5 Turbo',
    description: 'ç»å…¸æ¨¡å‹ï¼Œé€Ÿåº¦å¿«ï¼Œæˆæœ¬æœ€ä½',
    contextWindow: 16385,
    recommended: false
  }
}

const DEFAULT_MODEL = 'gpt-4o'
const API_BASE = 'https://api.openai.com/v1'

export class OpenAIProvider implements AIProvider {
  readonly type = 'openai' as const
  readonly meta: ProviderMeta = PROVIDER_INFO.openai

  private apiKey: string = ''
  private currentModel: string = DEFAULT_MODEL
  private initialized: boolean = false

  async init(apiKey: string, modelName?: string): Promise<boolean> {
    try {
      if (modelName && modelName in OPENAI_MODELS) {
        this.currentModel = modelName
      }

      this.apiKey = apiKey
      this.initialized = true
      console.log(`âœ… OpenAI initialized with model: ${this.currentModel}`)
      return true
    } catch (error) {
      console.error('âŒ Failed to initialize OpenAI:', error)
      return false
    }
  }

  async generateText(
    prompt: string,
    retries: number = 2,
    _timeout: number = 60000
  ): Promise<string> {
    if (!this.initialized || !this.apiKey) {
      throw new Error('OpenAI API æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆåœ¨è®¾ç½®ä¸­é…ç½® API Key')
    }

    let lastError: any = null

    for (let attempt = 0; attempt <= retries; attempt++) {
      try {
        console.log(`[OpenAI API] è¯·æ±‚å°è¯• ${attempt + 1}/${retries + 1}`)

        const response = await aiFetch(`${API_BASE}/chat/completions`, {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${this.apiKey}`
          },
          body: JSON.stringify({
            model: this.currentModel,
            messages: [{ role: 'user', content: prompt }],
            temperature: 0.7
          })
        })

        if (!response.ok) {
          const errorMsg = response.data?.error?.message || response.error || `HTTP ${response.status}`
          throw new Error(errorMsg)
        }

        const text = response.data?.choices?.[0]?.message?.content || ''

        console.log(`[OpenAI API] è¯·æ±‚æˆåŠŸ`)
        return text

      } catch (error: any) {
        lastError = error
        const errorMsg = error.message || String(error)
        console.error(`[OpenAI API] è¯·æ±‚å¤±è´¥ (å°è¯• ${attempt + 1}/${retries + 1}):`, errorMsg)

        if (attempt === retries) break

        if (errorMsg.includes('429') || errorMsg.includes('rate limit')) {
          throw new Error('âš ï¸ API é…é¢å·²ç”¨å°½ï¼Œè¯·ç¨åé‡è¯•')
        }

        if (errorMsg.includes('401') || errorMsg.includes('invalid')) {
          throw new Error('âŒ API Key æ— æ•ˆï¼Œè¯·æ£€æŸ¥å…¨å±€è®¾ç½®')
        }

        const waitTime = Math.min(2000 * (attempt + 1), 5000)
        console.log(`[OpenAI API] ${waitTime}ms åé‡è¯•...`)
        await new Promise(resolve => setTimeout(resolve, waitTime))
      }
    }

    const errorMsg = lastError?.message || String(lastError)
    if (errorMsg.includes('fetch') || errorMsg.includes('network')) {
      throw new Error('ğŸŒ ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè®¾ç½®')
    }

    throw new Error(`ç”Ÿæˆå¤±è´¥: ${errorMsg}`)
  }

  async *generateTextStream(prompt: string): AsyncGenerator<string, void, unknown> {
    if (!this.initialized || !this.apiKey) {
      throw new Error('OpenAI API æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆåœ¨è®¾ç½®ä¸­é…ç½® API Key')
    }

    const response = await fetch(`${API_BASE}/chat/completions`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${this.apiKey}`
      },
      body: JSON.stringify({
        model: this.currentModel,
        messages: [{ role: 'user', content: prompt }],
        temperature: 0.7,
        stream: true
      })
    })

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}))
      throw new Error(errorData.error?.message || `HTTP ${response.status}`)
    }

    const reader = response.body?.getReader()
    if (!reader) throw new Error('æ— æ³•è·å–å“åº”æµ')

    const decoder = new TextDecoder()
    let buffer = ''

    while (true) {
      const { done, value } = await reader.read()
      if (done) break

      buffer += decoder.decode(value, { stream: true })
      const lines = buffer.split('\n')
      buffer = lines.pop() || ''

      for (const line of lines) {
        if (line.startsWith('data: ')) {
          const data = line.slice(6)
          if (data === '[DONE]') return
          try {
            const json = JSON.parse(data)
            const content = json.choices?.[0]?.delta?.content
            if (content) yield content
          } catch {
            // Skip invalid JSON
          }
        }
      }
    }
  }

  async switchModel(modelName: string): Promise<boolean> {
    if (!this.initialized) {
      throw new Error('OpenAI API æœªåˆå§‹åŒ–')
    }

    if (!(modelName in OPENAI_MODELS)) {
      throw new Error(`ä¸æ”¯æŒçš„æ¨¡å‹: ${modelName}`)
    }

    this.currentModel = modelName
    console.log(`Switched to model: ${this.currentModel}`)
    return true
  }

  async checkQuota(): Promise<QuotaInfo> {
    if (!this.apiKey) {
      return {
        isValid: false,
        model: this.currentModel,
        error: 'API Key æœªé…ç½®'
      }
    }

    try {
      console.log(`[OpenAI] æ£€æŸ¥é…é¢ï¼Œä½¿ç”¨æ¨¡å‹: ${this.currentModel}`)
      console.log(`[OpenAI] API ç«¯ç‚¹: ${API_BASE}/chat/completions`)

      // å‘é€ä¸€ä¸ªç®€å•çš„æµ‹è¯•è¯·æ±‚ï¼ˆé€šè¿‡ IPC ä»£ç†ï¼‰
      const response = await aiFetch(`${API_BASE}/chat/completions`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${this.apiKey}`
        },
        body: JSON.stringify({
          model: this.currentModel,
          messages: [{ role: 'user', content: 'Hi' }],
          max_tokens: 5
        })
      })

      console.log(`[OpenAI] å“åº”çŠ¶æ€: ${response.status}`)

      if (response.ok) {
        return {
          isValid: true,
          model: this.currentModel,
          quotaExceeded: false
        }
      }

      const errorMsg = response.data?.error?.message || response.error || `HTTP ${response.status}`

      console.log(`[OpenAI] é”™è¯¯å“åº”:`, response.data)

      const isQuotaError = response.status === 429 ||
                           errorMsg.includes('rate limit') ||
                           errorMsg.includes('quota')

      return {
        isValid: false,
        model: this.currentModel,
        error: errorMsg,
        quotaExceeded: isQuotaError
      }
    } catch (error: any) {
      console.error(`[OpenAI] é…é¢æ£€æŸ¥å¼‚å¸¸:`, error)

      let errorMessage = error.message || String(error)
      if (errorMessage.includes('Failed to fetch') || errorMessage.includes('fetch')) {
        errorMessage = 'ç½‘ç»œè¿æ¥å¤±è´¥ã€‚å¦‚æœåœ¨ä¸­å›½å¤§é™†ï¼Œè¯·ç¡®ä¿å·²å¯ç”¨ä»£ç†å¹¶é‡å¯åº”ç”¨ã€‚'
      }

      return {
        isValid: false,
        model: this.currentModel,
        error: errorMessage
      }
    }
  }

  getAvailableModels(): Record<string, ModelInfo> {
    return OPENAI_MODELS
  }

  getCurrentModel(): string {
    return this.currentModel
  }

  isReady(): boolean {
    return this.initialized && !!this.apiKey
  }
}

// å¯¼å‡ºå•ä¾‹å®ä¾‹
export const openaiProvider = new OpenAIProvider()

// å¯¼å‡ºæ¨¡å‹é…ç½®
export { OPENAI_MODELS }
